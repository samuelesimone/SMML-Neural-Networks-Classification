\chapter{Neural Network Models} \label{ch:nnm}
In this chapter we are going to examine the models that have been made in order to solve the binary classification task.
All models were trained using the principle of K-Fold cross validation. In this case the \texttt{K = 5}. For the results of the experiments please refer to the Chapter \ref{ch:experiments} while for the discussion of the experiments please refer to the Chapter \ref{ch:discussion}.

\section{K-fold cross validation (K = 5)}
Cross-validation is a statistical method used to estimate the skill of machine learning models.The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model such as k=5 becoming 5-fold cross-validation, the one used in this project. \cite{kfold}.
The general rules are the following:
\begin{enumerate}
\item Shuffle the dataset randomly
\item Split the dataset into k groups
\item For each group:
\begin{itemize}
\item Take the group as a hold out or test data set
\item Take the remaining groups as a training data set
\item Fit a model on the training set and evaluate it on the test set
\item Retain the evaluation score and discard the model
\end{itemize}
\item Summarize the skill of the model using the sample of model evaluation scores
\end{enumerate}
Let's take a look at its mathematical formalization.
Let $S$ be our entire dataset \cite{kfoldmath}. We partition $S$ in $K$ subsets (also known as folds) $S_1, \ldots, S_K$ of size $m / K$ each (assume for simplicity that $K$ divides $m$).The $K$-fold $C V$ estimate of $\mathbb{E}\left[\ell_{\mathcal{D}}(A)\right]$ on $S$, denoted by $\ell_S^{\mathrm{cv}}(A)$, is then computed as follows: we run $A$ on each training part $S_{-i}$ of the folds $i=1, \ldots, K$ and obtain the predictors $h_1=$ $A\left(S_{-i}\right), \ldots, h_K=A\left(S_{-K}\right)$. We then compute the (rescaled) errors on the testing part of each fold,
$$
\ell_{S_i}\left(h_i\right)=\frac{K}{m} \sum_{(\boldsymbol{x}, y) \in S_i} \ell\left(y, h_i(\boldsymbol{x})\right)
$$
Finally, we compute the CV estimate by averaging these errors
$$
\ell_S^{\mathrm{cv}}(A)=\frac{1}{K} \sum_{i=1}^K \ell_{S_i}\left(h_i\right)
$$
\section{Multi-Layer Perceptron (MLP)}
A multilayer perceptron (MLP) is a feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable. \cite{mlp}
During the project, I developed 3 different MLP models so that I could conduct different experiments and understand the performance of them.
\subsection{First MLP Model}
Here are the MLP architecture:
\begin{itemize}
\item \texttt{inputs = tf.keras.Input(shape=input\_shape)}: model's input with the input\_shape size. In this case (128, 128, 3)
\item \texttt{x = layers.Flatten()(inputs)}: Converts 2D input (an image) to a 1D vector
\item \texttt{x = layers.Dense(256, activation='relu')(x)}: Fully connected layer (dense) with 256 neurons and ReLU activation
\item \texttt{x = layers.Dropout(0.5)(x)}: This dropout layer introduces regularization into the network. The parameter 0.5 indicates that 50\% of the neurons exiting this layer are randomly switched off during training.
\item \texttt{x = layers.Dense(128, activation='relu')(x)}: Other fully connected layer with 128 neurons and ReLU activation.
\item \texttt{x = layers.Dropout(0.5)(x)}: Another dropout layer for further regularization.
\item \texttt{activation = 'sigmoid'}: Here the activation function for the last layer is specified. In your case, you are using 'sigmoid,' which is commonly used in binary classification problems where the output must be a probability between 0 and 1 for each class.
\item \texttt{outputs = layers.Dense(num\_classes, activation=activation)(x)}. 
num\_classes = 1. This is the last layer of the model, which returns the output.
Then these are the technical details of the setup:
\begin{itemize}
\item \texttt{batch\_size =  32}. Indicates how many data examples are processed together before updating the weights.
\item \texttt{optimizer='adam'}. The optimizer is the algorithm that adjusts the model weights during training to minimize the cost function. It is a stochastic gradient descent method.
\item \texttt{loss = 'binary\_crossentropy'}. It specifies the cost function (or loss function) that will be used to evaluate how well the model is fitting the data during training. This cross-entropy loss is user for binary (0 or 1) classification applications.
\item \texttt{metrics=[zero\_one\_loss\_func]}: this param specifies the evaluation metrics that will be calculated during model training. In this case I used a custom function called \texttt{zero\_one\_loss\_func}. It requires two parameters: \texttt{y\_true} , the real associated label and the \texttt{y\_pred}, the ones predicted by the model. Then:
\begin{itemize}
\item Transforms predicted probabilities into binary labels (0 or 1)
\item Compare the predicted binary labels with the actual labels
\item Calculate the zero-one loss as the average of the errors.
\end{itemize}
\end{itemize}
\end{itemize}
\subsection{Hypertuning parameter for MLP}
Hyper-parameters are parameters that are not directly learnt within estimators.
In order to obtain better results I decided to do an hypertuning of the params using the \textbf{GridSearchCV} from the library \texttt{sklearn.model\_selection}.  GridSearchCV exhaustively considers all parameter combinations from a paramater grid \cite{scikitgs}.
Due the amount of the time spent for the exhaustive combination I choose just two different learning rate \texttt{0.001} and  \texttt{0.01}.
After wrapping the mlp keras into a KerasClassifier I can integrate the scikit-learn flow and run GridSearch. Here the results:
\begin{lstlisting}
Highest score is 0.5403 with {'learning_rate': 0.001, 'optimizer': <class 'keras.optimizers.adam.Adam'>} 

List in descending order:
0.5403 --> {'learning_rate': 0.001, 'optimizer': <class 'keras.optimizers.adam.Adam'>} 
0.5119 --> {'learning_rate': 0.01, 'optimizer': <class 'keras.optimizers.adam.Adam'>} 
\end{lstlisting}
So in this case with a learning rate of \texttt{0.001} the accuracy is better.
\subsection{First MLP Model after Hypertuning}
I trained the same mlp but with different learning rate as discussed above.
The results are quite similar as you can see from the Table \ref{table:mlp1hyp} so I spent some times to create other two mlp architecture.
\subsection{Second MLP Model}
In the second MLP Model I added two fully connected layer(dense) after the 0.5 Dropout of the first MLP Model.
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer (type)} & \textbf{Output Shape} & \textbf{Param \#} \\
\hline
input\_7 (InputLayer) & [(None, 128, 128, 3)] & 0 \\
\hline
flatten\_5 (Flatten) & (None, 49152) & 0 \\
\hline
dense\_15 (Dense) & (None, 256) & 12,583,168 \\
\hline
dropout\_10 (Dropout) & (None, 256) & 0 \\
\hline
dense\_16 (Dense) & (None, 128) & 32,896 \\
\hline
dropout\_11 (Dropout) & (None, 128) & 0 \\
\hline
dense\_17 (Dense) & (None, 64) & 8,256 \\
\hline
dropout\_12 (Dropout) & (None, 64) & 0 \\
\hline
dense\_18 (Dense) & (None, 32) & 2,080 \\
\hline
dropout\_13 (Dropout) & (None, 32) & 0 \\
\hline
dense\_19 (Dense) & (None, 1) & 33 \\
\hline
\multicolumn{3}{|c|}{Total params: 12,626,433} \\
\hline
\multicolumn{3}{|c|}{Trainable params: 12,626,433} \\
\hline
\multicolumn{3}{|c|}{Non-trainable params: 0} \\
\hline
\end{tabular}
\end{center}
Also in this case, the performance are not so high as reported in the Table. I executed a 10 epoch training and here the results:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Epoch} & \textbf{Loss} & \textbf{Accuracy} \\
\hline
1/10 & 1083.8823 & 0.4843 \\
\hline
2/10 & 10.3901 & 0.5398 \\
\hline
3/10 & 9.8943 & 0.5405 \\
\hline
4/10 & 9.4855 & 0.5398 \\
\hline
5/10 & 9.1378 & 0.5400 \\
\hline
6/10 & 8.7128 & 0.5405 \\
\hline
7/10 & 8.3303 & 0.5407 \\
\hline
8/10 & 7.9671 & 0.5407 \\
\hline
9/10 & 7.6214 & 0.5407 \\
\hline
\end{tabular}
\end{center}
\subsection{Third MLP Model}
The third MLP Model has 3 fully connected layers (512,256,128) with a Dropout for each layer of 0.3. Here the results:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Epoch & Loss & Accuracy \\
\hline
1 & 900.3068 & 0.5039 \\
\hline
2 & 27.8657 & 0.5172 \\
\hline
3 & 16.0389 & 0.5398 \\
\hline
4 & 14.6262 & 0.5394 \\
\hline
5 & 13.1948 & 0.5400 \\
\hline
6 & 12.0603 & 0.5407 \\
\hline
7 & 11.3178 & 0.5405 \\
\hline
8 & 10.7011 & 0.5394 \\
\hline
9 & 9.8406 & 0.5400 \\
\hline
10 & 9.1993 & 0.5400 \\
\hline
\end{tabular}
\end{center}

\section{Convolutional Neural Network (CNN)}
\section{Deep Residual learning (ResNet50)}